{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMIX SBX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "from snowflake.connector import connect #type: ignore\n",
    "\n",
    "from sklearn.model_selection import train_test_split #type: ignore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score \n",
    "\n",
    "from pandas import DataFrame, Series, merge, concat\n",
    "\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "conn = connect(\n",
    "    user = config.get('SNOWFLAKE', 'USER'),\n",
    "    password = config.get('SNOWFLAKE', 'PASSWORD'),\n",
    "    account = config.get('SNOWFLAKE', 'ACCOUNT'),\n",
    "    database = config.get('SNOWFLAKE', 'DATABASE'),\n",
    "    warehouse = config.get('SNOWFLAKE', 'WAREHOUSE'),\n",
    "    schema = config.get('SNOWFLAKE', 'SCHEMA'),\n",
    "    role = config.get('SNOWFLAKE', 'ROLE'),\n",
    ")\n",
    "\n",
    "FECHA_INICIO_ACTIVOS = \"'2023-06-12'\"\n",
    "FECHA_INICIO = \"'2023-09-04'\"\n",
    "FECHA_FIN = \"'2023-09-10'\"\n",
    "ANIO_MEDICION = 2023\n",
    "SEMANA_MEDICION = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados Journey Nuevos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ANIO_ALSEA  SEM_ALSEA  REDENCIONES  VENTA_CON_REDENCION\n",
      "0        2023         37           25          2372.413793\n",
      "1        2023         36          404         39293.534483\n",
      "2        2023         35          715         68775.431034\n",
      "3        2023         34          718         71605.603448\n",
      "4        2023         33          838         82298.275862\n"
     ]
    }
   ],
   "source": [
    "resultados_journey_nuevos = conn.cursor().execute(\"\"\"\n",
    "    WITH \n",
    "\n",
    "    ENVIADOS AS (\n",
    "        SELECT\n",
    "            FOLIO_CUPON,\n",
    "            USER_ID,\n",
    "            to_date(max(TIMESTAMP)) AS FECHA,\n",
    "            STEP\n",
    "        FROM\n",
    "            SEGMENT_EVENTS.HTTP_REGISTRA_CUPON.COUPON_RECORD AS ENVIADOS\n",
    "        WHERE\n",
    "            SOURCE = 'SBX Journey Nuevos'\n",
    "        GROUP BY\n",
    "            FOLIO_CUPON,\n",
    "            USER_ID,\n",
    "            STEP\n",
    "        HAVING\n",
    "            FECHA >= to_date('2023-07-10')\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        ANIO_ALSEA,\n",
    "        SEM_ALSEA,\n",
    "        count(REDIMIDOS.MONTO_VENTA) AS REDENCIONES,\n",
    "        sum(REDIMIDOS.MONTO_VENTA) / 1.16 AS VENTA_CON_REDENCION\n",
    "    FROM\n",
    "        ENVIADOS \n",
    "    LEFT JOIN\n",
    "        SEGMENT_EVENTS.CUPONERA_ALSEA_PROD.REDEMPTION_COUPON AS REDIMIDOS\n",
    "    ON\n",
    "        ENVIADOS.FOLIO_CUPON = REDIMIDOS.ID_COUPON\n",
    "    INNER JOIN\n",
    "        WOW_REWARDS.WORK_SPACE_WOW_REWARDS.DS_DIM_TIME\n",
    "    ON\n",
    "        to_date(REDIMIDOS.TIMESTAMP) = DS_DIM_TIME.FECHA\n",
    "    GROUP BY\n",
    "        ANIO_ALSEA,\n",
    "        SEM_ALSEA\n",
    "    ORDER BY\n",
    "        ANIO_ALSEA DESC,\n",
    "        SEM_ALSEA DESC\n",
    "    ;\n",
    "\"\"\")\n",
    "\n",
    "resultados_journey_nuevos = resultados_journey_nuevos.fetch_pandas_all()\n",
    "\n",
    "print(resultados_journey_nuevos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados WIFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ANIO_ALSEA  SEM_ALSEA  USR_PRIMERA_VENTA               VENTAS\n",
      "0        2023         37                 57    9641.379310344827\n",
      "1        2023         36               1025  137690.250000000004\n",
      "2        2023         35                990  142053.750000000003\n",
      "3        2023         34                939  134704.310344827593\n",
      "4        2023         33               1201  169754.991379310330\n"
     ]
    }
   ],
   "source": [
    "resultados_wifi = conn.cursor().execute(\"\"\"\n",
    "    WITH\n",
    "\n",
    "    NOSR AS (\n",
    "        SELECT DISTINCT \n",
    "            PROGRAM_WELCOMING.EMAIL,\n",
    "            to_date(dateadd(hour, -5, PROGRAM_WELCOMING.UUID_TS)) AS CREATED\n",
    "        FROM \n",
    "            SEGMENT_EVENTS.PERSONAS_STARBUCKS_PERSONAS.IDENTIFIES\n",
    "        INNER JOIN \n",
    "            SEGMENT_EVENTS.COREALSEASBX_PROD.PROGRAM_WELCOMING \n",
    "        ON\n",
    "            lower(PROGRAM_WELCOMING.EMAIL) = lower(IDENTIFIES.USER_ID)\n",
    "        WHERE \n",
    "            IDENTIFIES.CONTEXT_PERSONAS_COMPUTATION_KEY LIKE ('j_o_sbx_02_neowifi_v_%')\n",
    "    ),\n",
    "\n",
    "    PRIMERA_VENTA AS (\n",
    "        SELECT DISTINCT\n",
    "            EMAIL,\n",
    "            first_value(CHECK_AMOUNT / 1.16) OVER (PARTITION BY EMAIL ORDER BY TRANSACTION_DATE) AS VENTAS,\n",
    "            first_value(TRANSACTION_DATE)    OVER (PARTITION BY EMAIL ORDER BY TRANSACTION_DATE) AS FECHA\n",
    "        FROM \n",
    "            \"SEGMENT_EVENTS\".\"SESSIONM_SBX\".\"SM_TRANSACTION_HEADERS\" TXN\n",
    "        INNER JOIN \n",
    "        (\n",
    "            SELECT DISTINCT \n",
    "                TRANSACTION_ID, \n",
    "                USER_ID\n",
    "            FROM \n",
    "                \"SEGMENT_EVENTS\".\"SESSIONM_SBX\".\"SM_TRANSACTION_PAYMENTS\"\n",
    "        ) AS Q\n",
    "        USING(\n",
    "            TRANSACTION_ID\n",
    "        )\n",
    "        INNER JOIN\n",
    "            SEGMENT_EVENTS.SESSIONM_SBX.SM_USERS\n",
    "        USING(\n",
    "            USER_ID\n",
    "        )\n",
    "        WHERE \n",
    "            TXN.CHANNEL<> 'API'\n",
    "        AND\n",
    "            TXN.TRANSACTION_ID NOT IN \n",
    "            (\n",
    "                SELECT DISTINCT  \n",
    "                    TRANSACTION_ID \n",
    "                FROM \n",
    "                    SEGMENT_EVENTS.SESSIONM_SBX.SM_TRANSACTION_ITEMS\n",
    "                WHERE \n",
    "                    POS_ITEM_KEY IN (\n",
    "                        'MEXLEG_106_sbx',\n",
    "                        'MEXLEG_109_sbx'\n",
    "                    ) \n",
    "            )\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "        ANIO_ALSEA,\n",
    "        SEM_ALSEA,\n",
    "        count(DISTINCT PRIMERA_VENTA.EMAIL) AS USR_PRIMERA_VENTA,\n",
    "        sum(PRIMERA_VENTA.VENTAS) AS VENTAS\n",
    "    FROM \n",
    "        NOSR\n",
    "    INNER JOIN\n",
    "        PRIMERA_VENTA\n",
    "    ON\n",
    "        lower(NOSR.EMAIL) = lower(PRIMERA_VENTA.EMAIL)\n",
    "    INNER JOIN\n",
    "        WOW_REWARDS.WORK_SPACE_WOW_REWARDS.DS_DIM_TIME AS TIME\n",
    "    ON \n",
    "        to_date(PRIMERA_VENTA.FECHA) = TIME.FECHA\n",
    "    GROUP BY\n",
    "        ANIO_ALSEA,\n",
    "        SEM_ALSEA\n",
    "    ORDER BY \n",
    "        ANIO_ALSEA DESC,\n",
    "        SEM_ALSEA DESC\n",
    "    ;\n",
    "\"\"\")\n",
    "\n",
    "resultados_wifi = resultados_wifi.fetch_pandas_all()\n",
    "\n",
    "print(resultados_wifi.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venta incremental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos la segmentacion que actualmente existe en sessionM, junto con sus datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor().execute(f\"\"\"\n",
    "    WITH\n",
    "\n",
    "    SEGMENTOS AS (\n",
    "        SELECT\n",
    "            SEGMENTO,\n",
    "            EMAIL\n",
    "        FROM\n",
    "            SEGMENT_EVENTS.SESSIONM_SBX.SBX_SEGMENTOS_20230803\n",
    "        INNER JOIN\n",
    "            SEGMENT_EVENTS.SESSIONM_SBX.SM_USERS\n",
    "        USING(\n",
    "            USER_ID\n",
    "        )\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "        SUM(CHECK_AMOUNT/1.16) AS VTA, \n",
    "        COUNT(DISTINCT TRANSACTION_ID) AS ORD,\n",
    "        to_date({FECHA_FIN}) - to_date(max(CREATED_AT)) AS RECENCY,\n",
    "        SEGMENTO\n",
    "    FROM \n",
    "        SEGMENT_EVENTS.SESSIONM_SBX.FACT_TRANSACTIONS \n",
    "    INNER JOIN\n",
    "        SEGMENTOS\n",
    "    ON\n",
    "        SEGMENTOS.EMAIL = FACT_TRANSACTIONS.EMAIL\n",
    "    WHERE \n",
    "        TRANSACTION_DATE BETWEEN to_date({FECHA_INICIO}) AND to_date({FECHA_FIN})\n",
    "    GROUP BY\n",
    "        FACT_TRANSACTIONS.EMAIL,\n",
    "        SEGMENTO\n",
    "    ;\n",
    "\"\"\")\n",
    "\n",
    "df = cur.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos los datos de los usuarios que no tienen segmento asignado en sessionM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_unlabeled = conn.cursor().execute(f\"\"\"\n",
    "    WITH\n",
    "\n",
    "    SEGMENTOS AS (\n",
    "        SELECT\n",
    "            SEGMENTO,\n",
    "            EMAIL\n",
    "        FROM\n",
    "            SEGMENT_EVENTS.SESSIONM_SBX.SBX_SEGMENTOS_20230803\n",
    "        INNER JOIN\n",
    "            SEGMENT_EVENTS.SESSIONM_SBX.SM_USERS\n",
    "        USING(\n",
    "            USER_ID\n",
    "        )\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "        FACT_TRANSACTIONS.EMAIL,\n",
    "        SUM(CHECK_AMOUNT/1.16) AS VTA, \n",
    "        COUNT(DISTINCT TRANSACTION_ID) AS ORD,\n",
    "        to_date({FECHA_FIN}) - to_date(max(CREATED_AT)) AS RECENCY\n",
    "    FROM \n",
    "        SEGMENT_EVENTS.SESSIONM_SBX.FACT_TRANSACTIONS \n",
    "    LEFT JOIN\n",
    "        SEGMENTOS\n",
    "    ON\n",
    "        SEGMENTOS.EMAIL = FACT_TRANSACTIONS.EMAIL\n",
    "    WHERE \n",
    "        TRANSACTION_DATE BETWEEN to_date({FECHA_INICIO}) AND to_date({FECHA_FIN})\n",
    "    AND\n",
    "        SEGMENTOS.EMAIL IS null\n",
    "    GROUP BY\n",
    "        FACT_TRANSACTIONS.EMAIL,\n",
    "        SEGMENTO\n",
    "    ;\n",
    "\"\"\")\n",
    "\n",
    "df_unlabeled = cur_unlabeled.fetch_pandas_all() #type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados antes de dividir por segmento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLV grupo promocion:239.13066496975895\n",
      "CLV grupo control:226.27697217123307\n",
      "Lift:0.05680512990424402\n",
      "Venta incremental:3349097.8191759014\n"
     ]
    }
   ],
   "source": [
    "clv_promocion = df['VTA'].mean()\n",
    "\n",
    "clv_control = df_unlabeled['VTA'].mean()\n",
    "\n",
    "lift = (clv_promocion / clv_control) - 1\n",
    "\n",
    "venta_grupo_promocion = df['VTA'].sum()\n",
    "\n",
    "venta_incremental = lift * int(venta_grupo_promocion)\n",
    "\n",
    "print(f'CLV grupo promocion:{clv_promocion}')\n",
    "\n",
    "print(f'CLV grupo control:{clv_control}')\n",
    "\n",
    "print(f'Lift:{lift}')\n",
    "\n",
    "print(f'Venta incremental:{venta_incremental}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el clasificador y obtenemos los resultados por segmento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         CLV_PROMOCION  CLV_CONTROL      LIFT VENTA_INCREMENTAL_TOTAL\n",
      "SEGMENTO                                                             \n",
      "HIGH        268.125853   253.089846   0.05941          2263159.314886\n",
      "LOW         167.783659   165.822142  0.011829             112826.7164\n",
      "MEDIUM      190.745946   187.662337  0.016432           363705.635758\n",
      "TOP         433.896021   413.518398  0.049279           609406.152133\n"
     ]
    }
   ],
   "source": [
    "x_unlabeled = df_unlabeled.drop('EMAIL', axis = 1)\n",
    "\n",
    "x:DataFrame = df.drop('SEGMENTO', axis = 1)\n",
    "y = df['SEGMENTO']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "x_unlabeled = scaler.transform(x_unlabeled)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "knn.fit(x, y)\n",
    "\n",
    "prediction = DataFrame(knn.predict(x_unlabeled))\n",
    "\n",
    "df_unlabeled['SEGMENTO'] = prediction[0]\n",
    "\n",
    "clv_promocion_por_segmento = df.groupby('SEGMENTO').agg({'VTA': 'mean'})\n",
    "\n",
    "venta_promocion_por_segmento = df.groupby('SEGMENTO').agg({'VTA': 'sum'})\n",
    "\n",
    "clv_control_por_segmento = df_unlabeled.groupby('SEGMENTO').agg({'VTA': 'mean'})\n",
    "\n",
    "merged = merge(clv_promocion_por_segmento, clv_control_por_segmento, on = 'SEGMENTO')\n",
    "\n",
    "merged = merged.rename(columns = {'VTA_x' : 'CLV_PROMOCION', 'VTA_y' : 'CLV_CONTROL'})\n",
    "\n",
    "merged['CLV_CONTROL'] = merged['CLV_CONTROL'].apply(lambda x : float(x))\n",
    "\n",
    "merged['LIFT'] = (merged['CLV_PROMOCION'] - merged['CLV_CONTROL']) / merged['CLV_CONTROL']\n",
    "\n",
    "merged = merge(merged, venta_promocion_por_segmento, on = 'SEGMENTO')\n",
    "\n",
    "merged['VTA'] = merged['VTA'].apply(lambda x : float(x))\n",
    "\n",
    "merged['VENTA_INCREMENTAL'] = merged['VTA'] * merged['LIFT']\n",
    "\n",
    "venta_incremental_asignada = merged['VENTA_INCREMENTAL'].sum()\n",
    "venta_incremental_por_asignar = venta_incremental - venta_incremental_asignada\n",
    "\n",
    "merged['PORCENTAJE_VENTA_INCREMENTAL'] = merged['VENTA_INCREMENTAL'] / venta_incremental_asignada \n",
    "\n",
    "merged['VENTA_INCREMENTAL_TOTAL'] = merged['VENTA_INCREMENTAL'] + (venta_incremental_por_asignar * merged['PORCENTAJE_VENTA_INCREMENTAL'])\n",
    "\n",
    "print(merged[['CLV_PROMOCION', 'CLV_CONTROL', 'LIFT', 'VENTA_INCREMENTAL_TOTAL']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medimos los otros datos por segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              VENTAS  ORDENES  USUARIOS  USUARIOS_TOTALES_SEGMENTACION  \\\n",
      "0   9544309.99138032    81986     68832                         440357   \n",
      "1  25762550.26724365   202566    139655                         347867   \n",
      "2  43340818.63794840   436856    156364                         205983   \n",
      "3  13060357.62069469    65991     26338                          33034   \n",
      "\n",
      "  ULTRASEGMENTO               CLV   TICKET_PROMEDIO FRECUENCIA  \n",
      "0           LOW  138.660942459616  116.413899828999   1.191103  \n",
      "1        MEDIUM  184.472809904720  127.181018864191   1.450474  \n",
      "2          HIGH  277.179009477555   99.210766563692   2.793840  \n",
      "3           TOP  495.875071026452  197.911194264289   2.505543  \n"
     ]
    }
   ],
   "source": [
    "cur_otros_datos_por_segmento = conn.cursor().execute(f\"\"\"\n",
    "    WITH\n",
    "\n",
    "    INTERVALO_FECHAS AS (\n",
    "        -- Fecha de inicio y final que vamos a usar para la segmentacion\n",
    "        SELECT\n",
    "            to_date({FECHA_INICIO_ACTIVOS}) AS FECHA_INICIO,\n",
    "            to_date({FECHA_FIN}) AS FECHA_FIN\n",
    "    ),\n",
    "\n",
    "    TRANSACCIONES_BY_USER AS (\n",
    "        SELECT\n",
    "            EMAIL,\n",
    "            sum(CHECK_AMOUNT / 1.16) AS VENTAS,\n",
    "            count(DISTINCT TRANSACTION_ID) AS FREQ,\n",
    "            VENTAS / FREQ AS AOV\n",
    "        FROM\n",
    "            SEGMENT_EVENTS.SESSIONM_SBX.FACT_TRANSACTIONS\n",
    "        INNER JOIN\n",
    "            INTERVALO_FECHAS\n",
    "        ON\n",
    "            to_date(CREATED_AT) BETWEEN FECHA_INICIO AND FECHA_FIN\n",
    "        WHERE\n",
    "            EMAIL IS NOT null\n",
    "        GROUP BY\n",
    "            EMAIL\n",
    "    ),\n",
    "\n",
    "    SEGMENTACION_BASE_ACTIVOS AS (\n",
    "        SELECT \n",
    "            EMAIL,\n",
    "            CASE\n",
    "                WHEN FREQ <= PERCENTILE_CONT(0.20) WITHIN GROUP (ORDER BY FREQ) OVER () THEN 'LIGHT'\n",
    "                WHEN FREQ <= PERCENTILE_CONT(0.40) WITHIN GROUP (ORDER BY FREQ) OVER () THEN 'MIDL'\n",
    "                WHEN FREQ <= PERCENTILE_CONT(0.60) WITHIN GROUP (ORDER BY FREQ) OVER () THEN 'MIDH'\n",
    "                WHEN FREQ <= PERCENTILE_CONT(0.80) WITHIN GROUP (ORDER BY FREQ) OVER () THEN 'HEAVY'\n",
    "                ELSE 'SUPER'\n",
    "            END AS SEGMENTO,\n",
    "            CASE\n",
    "                WHEN AOV <= ROUND(PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY AOV) OVER ()) THEN 'LOW'\n",
    "                WHEN AOV <= ROUND(PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY AOV) OVER ()) THEN 'AVG'\n",
    "                ELSE 'HIGH'\n",
    "            END AS TAG_GRUPO,\n",
    "            SEGMENTO || '_' || TAG_GRUPO AS FULL_SEGMENTO,\n",
    "            CASE\n",
    "                WHEN FULL_SEGMENTO IN ('LIGHT_LOW', 'LIGHT_AVG', 'LIGHT_HIGH', 'MIDL_LOW', 'MIDL_AVG', 'MIDH_LOW') THEN 'LOW'\n",
    "                WHEN FULL_SEGMENTO IN ('MIDL_HIGH', 'MIDH_AVG', 'MIDH_HIGH', 'HEAVY_LOW', 'HEAVY_AVG') THEN 'MEDIUM' \n",
    "                WHEN FULL_SEGMENTO IN ('HEAVY_HIGH', 'SUPER_LOW', 'SUPER_AVG') THEN 'HIGH'\n",
    "                ELSE 'TOP'\n",
    "            END AS ULTRASEGMENTO\n",
    "        FROM\n",
    "            TRANSACCIONES_BY_USER\n",
    "    ),\n",
    "\n",
    "    TRANSACCIONES_TOTALES AS (\n",
    "        SELECT \n",
    "            SEM_ALSEA, \n",
    "            ANIO_ALSEA,\n",
    "            CHECK_AMOUNT/1.16 AS VENTA, \n",
    "            TRANSACTION_ID,\n",
    "            EMAIL\n",
    "        FROM \n",
    "            \"SEGMENT_EVENTS\".\"SESSIONM_SBX\".\"FACT_TRANSACTIONS\" A\n",
    "        INNER JOIN \n",
    "            \"WOW_REWARDS\".\"WORK_SPACE_WOW_REWARDS\".\"DS_DIM_TIME\" T\n",
    "        ON \n",
    "            to_date(CREATED_AT) = T.FECHA\n",
    "        WHERE\n",
    "            ANIO_ALSEA = {ANIO_MEDICION}\n",
    "        AND\n",
    "            SEM_ALSEA = {SEMANA_MEDICION}    \n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        sum(VENTA) AS VENTAS,\n",
    "        count(DISTINCT TRANSACTION_ID) AS ORDENES,\n",
    "        count(DISTINCT TRANSACCIONES_TOTALES.EMAIL) AS USUARIOS,\n",
    "        count(DISTINCT SEGMENTACION.EMAIL) AS USUARIOS_TOTALES_SEGMENTACION,\n",
    "        ULTRASEGMENTO,\n",
    "        VENTAS/USUARIOS AS CLV,\n",
    "        VENTAS/ORDENES AS TICKET_PROMEDIO,\n",
    "        ORDENES/USUARIOS AS FRECUENCIA\n",
    "    FROM\n",
    "        SEGMENTACION_BASE_ACTIVOS AS SEGMENTACION\n",
    "    LEFT JOIN\n",
    "        TRANSACCIONES_TOTALES\n",
    "    ON\n",
    "        SEGMENTACION.EMAIL = TRANSACCIONES_TOTALES.EMAIL\n",
    "    GROUP BY\n",
    "        ULTRASEGMENTO\n",
    "    ORDER BY\n",
    "        CASE ULTRASEGMENTO\n",
    "            WHEN 'LOW' THEN 1\n",
    "            WHEN 'MEDIUM' THEN 2\n",
    "            WHEN 'HIGH' THEN 3\n",
    "            WHEN 'TOP' THEN 4\n",
    "        END\n",
    "    ;\n",
    "\"\"\")\n",
    "\n",
    "otros_datos_por_segmento = cur_otros_datos_por_segmento.fetch_pandas_all()\n",
    "print(otros_datos_por_segmento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medimos los otros datos sin segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              VENTAS  ORDENES  USUARIOS  USUARIOS_TOTALES_SEGMENTACION  \\\n",
      "0  91708036.51726706   787399    391189                        1027241   \n",
      "\n",
      "  ULTRASEGMENTO               CLV   TICKET_PROMEDIO FRECUENCIA  \n",
      "0         TOTAL  234.434088170340  116.469587232479   2.012835  \n"
     ]
    }
   ],
   "source": [
    "cur_otros_datos_sin_segmento = conn.cursor().execute(f\"\"\"\n",
    "    WITH\n",
    "\n",
    "    INTERVALO_FECHAS AS (\n",
    "        -- Fecha de inicio y final que vamos a usar para la segmentacion\n",
    "        SELECT\n",
    "            to_date({FECHA_INICIO_ACTIVOS}) AS FECHA_INICIO,\n",
    "            to_date({FECHA_FIN}) AS FECHA_FIN\n",
    "    ),\n",
    "\n",
    "    TRANSACCIONES_BY_USER AS (\n",
    "        SELECT\n",
    "            EMAIL,\n",
    "            sum(CHECK_AMOUNT / 1.16) AS VENTAS,\n",
    "            count(DISTINCT TRANSACTION_ID) AS FREQ,\n",
    "            VENTAS / FREQ AS AOV\n",
    "        FROM\n",
    "            SEGMENT_EVENTS.SESSIONM_SBX.FACT_TRANSACTIONS\n",
    "        INNER JOIN\n",
    "            INTERVALO_FECHAS\n",
    "        ON\n",
    "            to_date(CREATED_AT) BETWEEN FECHA_INICIO AND FECHA_FIN\n",
    "        WHERE\n",
    "            EMAIL IS NOT null\n",
    "        GROUP BY\n",
    "            EMAIL\n",
    "    ),\n",
    "\n",
    "    SEGMENTACION_BASE_ACTIVOS AS (\n",
    "        SELECT \n",
    "            EMAIL,\n",
    "            CASE\n",
    "                WHEN FREQ <= PERCENTILE_CONT(0.20) WITHIN GROUP (ORDER BY FREQ) OVER () THEN 'LIGHT'\n",
    "                WHEN FREQ <= PERCENTILE_CONT(0.40) WITHIN GROUP (ORDER BY FREQ) OVER () THEN 'MIDL'\n",
    "                WHEN FREQ <= PERCENTILE_CONT(0.60) WITHIN GROUP (ORDER BY FREQ) OVER () THEN 'MIDH'\n",
    "                WHEN FREQ <= PERCENTILE_CONT(0.80) WITHIN GROUP (ORDER BY FREQ) OVER () THEN 'HEAVY'\n",
    "                ELSE 'SUPER'\n",
    "            END AS SEGMENTO,\n",
    "            CASE\n",
    "                WHEN AOV <= ROUND(PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY AOV) OVER ()) THEN 'LOW'\n",
    "                WHEN AOV <= ROUND(PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY AOV) OVER ()) THEN 'AVG'\n",
    "                ELSE 'HIGH'\n",
    "            END AS TAG_GRUPO,\n",
    "            SEGMENTO || '_' || TAG_GRUPO AS FULL_SEGMENTO,\n",
    "            CASE\n",
    "                WHEN FULL_SEGMENTO IN ('LIGHT_LOW', 'LIGHT_AVG', 'LIGHT_HIGH', 'MIDL_LOW', 'MIDL_AVG', 'MIDH_LOW') THEN 'LOW'\n",
    "                WHEN FULL_SEGMENTO IN ('MIDL_HIGH', 'MIDH_AVG', 'MIDH_HIGH', 'HEAVY_LOW', 'HEAVY_AVG') THEN 'MEDIUM' \n",
    "                WHEN FULL_SEGMENTO IN ('HEAVY_HIGH', 'SUPER_LOW', 'SUPER_AVG') THEN 'HIGH'\n",
    "                ELSE 'TOP'\n",
    "            END AS ULTRASEGMENTO\n",
    "        FROM\n",
    "            TRANSACCIONES_BY_USER\n",
    "    ),\n",
    "\n",
    "    TRANSACCIONES_TOTALES AS (\n",
    "        SELECT \n",
    "            SEM_ALSEA, \n",
    "            ANIO_ALSEA,\n",
    "            CHECK_AMOUNT/1.16 AS VENTA, \n",
    "            TRANSACTION_ID,\n",
    "            EMAIL\n",
    "        FROM \n",
    "            \"SEGMENT_EVENTS\".\"SESSIONM_SBX\".\"FACT_TRANSACTIONS\" A\n",
    "        INNER JOIN \n",
    "            \"WOW_REWARDS\".\"WORK_SPACE_WOW_REWARDS\".\"DS_DIM_TIME\" T\n",
    "        ON \n",
    "            to_date(CREATED_AT) = T.FECHA\n",
    "        WHERE\n",
    "            ANIO_ALSEA = {ANIO_MEDICION}\n",
    "        AND\n",
    "            SEM_ALSEA = {SEMANA_MEDICION}    \n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        sum(VENTA) AS VENTAS,\n",
    "        count(DISTINCT TRANSACTION_ID) AS ORDENES,\n",
    "        count(DISTINCT TRANSACCIONES_TOTALES.EMAIL) AS USUARIOS,\n",
    "        count(DISTINCT SEGMENTACION.EMAIL) AS USUARIOS_TOTALES_SEGMENTACION,\n",
    "        'TOTAL' AS ULTRASEGMENTO,\n",
    "        VENTAS/USUARIOS AS CLV,\n",
    "        VENTAS/ORDENES AS TICKET_PROMEDIO,\n",
    "        ORDENES/USUARIOS AS FRECUENCIA\n",
    "    FROM\n",
    "        SEGMENTACION_BASE_ACTIVOS AS SEGMENTACION\n",
    "    LEFT JOIN\n",
    "        TRANSACCIONES_TOTALES\n",
    "    ON\n",
    "        SEGMENTACION.EMAIL = TRANSACCIONES_TOTALES.EMAIL\n",
    "    ;\n",
    "\"\"\")\n",
    "\n",
    "otros_datos_sin_segmento = cur_otros_datos_sin_segmento.fetch_pandas_all()\n",
    "print(otros_datos_sin_segmento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unimos los datos con y sin segmento, y los ponemos en un archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ULTRASEGMENTO  USUARIOS_TOTALES_SEGMENTACION   TICKET_PROMEDIO FRECUENCIA  \\\n",
      "0         TOTAL                        1027241  116.469587232479   2.012835   \n",
      "0           LOW                         440357  116.413899828999   1.191103   \n",
      "1        MEDIUM                         347867  127.181018864191   1.450474   \n",
      "2          HIGH                         205983   99.210766563692   2.793840   \n",
      "3           TOP                          33034  197.911194264289   2.505543   \n",
      "\n",
      "                CLV  \n",
      "0  234.434088170340  \n",
      "0  138.660942459616  \n",
      "1  184.472809904720  \n",
      "2  277.179009477555  \n",
      "3  495.875071026452  \n"
     ]
    }
   ],
   "source": [
    "otros_datos_total = concat([otros_datos_sin_segmento, otros_datos_por_segmento])[['ULTRASEGMENTO', 'USUARIOS_TOTALES_SEGMENTACION', 'TICKET_PROMEDIO', 'FRECUENCIA', 'CLV']]\n",
    "print(otros_datos_total)\n",
    "otros_datos_total.to_csv('results' + FECHA_FIN + '.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
